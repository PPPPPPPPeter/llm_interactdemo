{
  "name": "llm-intera-backend",
  "version": "1.0.0",
  "description": "Backend for LLM Intera demo",
  "main": "server.js",
  "scripts": {
    "start": "node server.js",
    "dev": "nodemon server.js"
  },
  "dependencies": {
    "@langchain/openai": "^1.1.1",
    "body-parser": "^1.20.3",
    "cors": "^2.8.5",
    "dotenv": "^17.2.3",
    "express": "^4.21.2",
    "langchain": "^1.0.4",
    "node-fetch": "^3.3.2"
  },
  "devDependencies": {
    "nodemon": "^3.0.1"
  },
  "keywords": [
    "backend",
    "api",
    "llm"
  ],
  "author": "Hongyu Wang",
  "license": "MIT"
}
